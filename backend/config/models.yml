# Default model configuration
# Supports separate defaults for different generation types
default_model: qwen-image
default_models:
  text_to_image: qwen-image
  image_to_image: qwen-image-edit

# Models configuration
models:
  # ==================== Qwen Image Models (Server Mode) ====================
  qwen-image:
    name: "Qwen Image"
    description: "Qwen Image - advanced text-to-image model with complex text rendering and precise image editing capabilities"
    default: true
    capabilities: ["text-to-image"]
    # Server mode command
    command: "./bin/sd-server"
    args:
      - "-l"
      - "127.0.0.1"
      - "--listen-port"
      - "1236"
      - "--diffusion-model"
      - "./models/Qwen_Image-Q4_K_M.gguf"
      - "--vae"
      - "./models/qwen_image_vae.safetensors"
      - "--qwen2vl"
      - "./models/Qwen2.5-VL-7B-Instruct.Q4_K_M.gguf"
      - "--cfg-scale"
      - "2.5"
      - "--sampling-method"
      - "euler"
      - "-v"
      - "--offload-to-cpu"
      - "--diffusion-fa"
      - "--flow-shift"
      - "3"
    api: "http://127.0.0.1:1236/v1"
    mode: "on_demand"
    exec_mode: "server"
    port: 1236
    model_type: "text-to-image"
    default_size: "1024x1024"
    huggingface:
      repo: "QuantStack/Qwen-Image-GGUF"
      files:
        - path: "Qwen_Image-Q4_K_M.gguf"
          dest: "./models/"
        - path: "qwen_image_vae.safetensors"
          dest: "./models/"
          repo: "Comfy-Org/Qwen-Image_ComfyUI"
          subpath: "split_files/vae"
        - path: "Qwen2.5-VL-7B-Instruct.Q4_K_M.gguf"
          dest: "./models/"
          repo: "mradermacher/Qwen2.5-VL-7B-Instruct-GGUF"

  qwen-image-edit:
    name: "Qwen Image Edit"
    description: "Qwen Image Edit - image-to-image editing model with object manipulation and style transformation"
    capabilities: ["image-to-image", "text-to-image"]
    command: "./bin/sd-server"
    args:
      - "-l"
      - "127.0.0.1"
      - "--listen-port"
      - "1237"
      - "--diffusion-model"
      - "./models/Qwen-Image-Edit-2509-Q4_K_M.gguf"
      - "--vae"
      - "./models/qwen_image_vae.safetensors"
      - "--qwen2vl"
      - "./models/Qwen2.5-VL-7B-Instruct.Q4_K_M.gguf"
      - "--cfg-scale"
      - "2.5"
      - "--sampling-method"
      - "euler"
      - "-v"
      - "--offload-to-cpu"
      - "--diffusion-fa"
      - "--flow-shift"
      - "3"
    api: "http://127.0.0.1:1237/v1"
    mode: "on_demand"
    exec_mode: "server"
    port: 1237
    model_type: "image-to-image"
    default_size: "1024x1024"
    huggingface:
      repo: "QuantStack/Qwen-Image-GGUF"
      files:
        - path: "Qwen-Image-Edit-2509-Q4_K_M.gguf"
          dest: "./models/"
        - path: "qwen_image_vae.safetensors"
          dest: "./models/"
          repo: "Comfy-Org/Qwen-Image_ComfyUI"
          subpath: "split_files/vae"
        - path: "Qwen2.5-VL-7B-Instruct.Q4_K_M.gguf"
          dest: "./models/"
          repo: "mradermacher/Qwen2.5-VL-7B-Instruct-GGUF"

  # ==================== Z-Image Turbo (CLI Mode) ====================
  # NOTE: Z-Image-Turbo is a distilled model that does not rely on classifier-free guidance.
  # Recommended settings: cfg_scale=0.0-1.0 (typically 0.0), steps=4-9 (typically 9)
  z-image-turbo:
    name: "Z-Image Turbo"
    description: "Z-Image Turbo - fast text-to-image model with 4GB VRAM support"
    capabilities: ["text-to-image"]
    mode: "on_demand"
    command: "./bin/sd-server"
    port: 1238
    args:
      - "-l"
      - "127.0.0.1"
      - "--listen-port"
      - "1238"
      - "--diffusion-model"
      - "./models/z_image_turbo-Q8_0.gguf"
      - "--vae"
      - "./models/ae.safetensors"
      - "--llm"
      - "./models/Qwen3-4B-Instruct-2507-Q8_0.gguf"
      - "-v"
      - "--offload-to-cpu"
      - "--diffusion-fa"
    api: null  # CLI mode
    exec_mode: "server"
    model_type: "text-to-image"
    default_size: "1024x1024"
    # Model-specific default parameters for z-image-turbo
    # These are applied when this model is selected
    generation_params:
      cfg_scale: 0.0  # Z-Image-Turbo doesn't rely on classifier-free guidance
      sample_steps: 9  # Ultra-fast distilled model, uses 4-9 steps
      sampling_method: "euler"
    huggingface:
      repo: "leejet/Z-Image-Turbo-GGUF"
      files:
        - path: "z_image_turbo-Q8_0.gguf"
          dest: "./models/"
        - path: "ae.safetensors"
          dest: "./models/"
          repo: "black-forest-labs/FLUX.1-schnell"
        - path: "Qwen3-4B-Instruct-2507-Q8_0.gguf"
          dest: "./models/"
          repo: "unsloth/Qwen3-4B-Instruct-2507-GGUF"

  # ==================== Copax Timeless XL+Z1 (CLI Mode) ====================
  # NOTE: Based on Z-Image-Turbo, uses distilled settings.
  # Recommended: cfg_scale=1.0, steps=14, sampling=res_2s
  copax-timeless-xlz1:
    name: "Copax Timeless XL+Z1"
    description: "Copax Timeless XL+Z1 - SDXL/Z-Image hybrid model for high quality outputs"
    capabilities: ["text-to-image"]
    command: "./bin/sd-cli"
    args:
      - "--diffusion-model"
      - "./models/copaxTimeless_xplusZ1.safetensors"
      - "--vae"
      - "./models/ae.safetensors"
      - "--llm"
      - "./models/Qwen3-4B-Instruct-2507-Q8_0.gguf"
      - "-p"
      - "A cinematic photograph"
      - "-v"
      - "--offload-to-cpu"
      - "--diffusion-fa"
      - "-H"
      - "1024"
      - "-W"
      - "1024"
    api: null  # CLI mode
    mode: "on_demand"
    exec_mode: "cli"
    model_type: "text-to-image"
    default_size: "1024x1024"
    # Model-specific default parameters for copax-timeless-xlz1
    # Based on Z-Image-Turbo distilled settings
    generation_params:
      cfg_scale: 1.0  # Distilled CFG Scale
      sample_steps: 14  # Recommended steps
      sampling_method: "res_2s"  # Recommended sampling method
    huggingface:
      repo: "copax/CopaxTimelessXLZ1"
      files:
        - path: "copaxTimeless_xplusZ1.safetensors"
          dest: "./models/"
        - path: "ae.safetensors"
          dest: "./models/"
          repo: "black-forest-labs/FLUX.1-schnell"
        - path: "Qwen3-4B-Instruct-2507-Q8_0.gguf"
          dest: "./models/"
          repo: "unsloth/Qwen3-4B-Instruct-2507-GGUF"
    # Civitai download source
    civitai:
      model_id: "2466559"
      download_url: "https://civitai.com/api/download/models/2466559?type=Model&format=SafeTensor&size=pruned&fp=fp8"

  # ==================== Flux Models ====================
  fluxed-up-flux:
    name: "FluxedUp Flux NSFW"
    description: "High quality flux model with NSFW support"
    capabilities: ["text-to-image"]
    command: "./bin/sd-server"
    args:
      - "-l"
      - "0.0.0.0"
      - "--diffusion-model"
      - "./models/fluxedUpFluxNSFW_v51Q4KSV2.gguf"
      - "--vae"
      - "./models/ae.safetensors"
      - "--clip_l"
      - "./models/clip_l.safetensors"
      - "--t5xxl"
      - "./models/t5xxl_fp16.safetensors"
      - "-p"
      - "a lovely cat holding a sign says 'flux.cpp'"
      - "--cfg-scale"
      - "1.0"
      - "--sampling-method"
      - "euler"
      - "-v"
      - "--clip-on-cpu"
    api: "http://localhost:1234/v1"
    mode: "on_demand"
    exec_mode: "server"
    port: 1234
    model_type: "text-to-image"
    default_size: "1024x1024"
    huggingface:
      repo: "gel基础/fluxedUpFluxNSFW_v51Q4KSV2"
      files:
        - path: "fluxedUpFluxNSFW_v51Q4KSV2.gguf"
          dest: "./models/"

  # ==================== Stable Diffusion Models ====================
  sd15-base:
    name: "SD 1.5 Base"
    description: "Stable Diffusion 1.5 base model"
    capabilities: ["text-to-image"]
    command: "./bin/sd-server"
    args:
      - "-l"
      - "0.0.0.0"
      - "--model"
      - "./models/v1-5-pruned-emaonly.ckpt"
      - "--port"
      - "1235"
    api: "http://localhost:1235/v1"
    mode: "preload"
    exec_mode: "server"
    port: 1235
    model_type: "text-to-image"
    default_size: "512x512"
    huggingface:
      repo: "runwayml/stable-diffusion-v1-5"
      files:
        - path: "v1-5-pruned-emaonly.ckpt"
          dest: "./models/"

  sdxl-turbo:
    name: "SDXL Turbo"
    description: "Fast SDXL Turbo for quick generations"
    capabilities: ["text-to-image"]
    command: "./bin/sd-cli"
    args:
      - "-m"
      - "./models/sd_xl_turbo_1.0_fp16.safetensors"
    api: null
    mode: "on_demand"
    exec_mode: "cli"
    model_type: "text-to-image"
    default_size: "512x512"
    huggingface:
      repo: "stabilityai/sdxl-turbo"
      files:
        - path: "sd_xl_turbo_1.0_fp16.safetensors"
          dest: "./models/"

  # ==================== External API Models ====================
  # Example configuration for using external OpenAI-compatible APIs
  # No local server is started - requests go directly to the external API
  flux2:
    name: "black-forest-labs/FLUX.2-pro"
    description: "External API"
    capabilities: ["text-to-image"]
    command: null  # No command needed for API mode
    args: []
    api: ""  # External API endpoint
    api_key: ""
    mode: "on_demand"  # Load strategy - always on demand for API mode
    exec_mode: "api"  # Execution mode - no local server
    model_type: "text-to-image"
    default_size: "1024x1024"

# ==================== Upscalers ====================
# Upscaler models for image enhancement
upscalers:
  realesrgan-x4plus:
    name: "RealESRGAN 4x+"
    description: "RealESRGAN x4 upscaler for general photo enhancement"
    model_path: "./models/RealESRGAN_x4plus.pth"
    model_name: "RealESRGAN"
    scale: 4
    type: "realesrgan"

  resize-lanczos:
    name: "Resize Lanczos"
    description: "Lanczos resampling algorithm for basic upscaling"
    model_name: "Resize"
    scale: 1  # Variable scale
    type: "resize"
    method: "lanczos"

  resize-bicubic:
    name: "Resize Bicubic"
    description: "Bicubic interpolation for basic upscaling"
    model_name: "Resize"
    scale: 1  # Variable scale
    type: "resize"
    method: "bicubic"

  resize-nearest:
    name: "Resize Nearest"
    description: "Nearest neighbor interpolation for pixel art upscaling"
    model_name: "Resize"
    scale: 1  # Variable scale
    type: "resize"
    method: "nearest"
